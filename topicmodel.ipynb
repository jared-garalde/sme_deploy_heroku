{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import pymssql\n",
    "# from fuzzywuzzy import fuzz\n",
    "import json\n",
    "import tweepy\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import re\n",
    "# import pyodbc\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import string, nltk, re, json, tweepy, gensim, scipy.sparse, pickle, pyLDAvis, pyLDAvis.gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import matutils, models, corpora\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "source": [
    "# Topic Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "df = pd.read_csv('./meme_cleaning.csv')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "IPyKernel not installed into interpreter Python 3.8.3 64-bit:/Library/Frameworks/Python.framework/Versions/3.8/bin/python3",
     "traceback": [
      "Error: IPyKernel not installed into interpreter Python 3.8.3 64-bit:/Library/Frameworks/Python.framework/Versions/3.8/bin/python3",
      "at L.installDependenciesIntoInterpreter (/Users/jaredgaralde/.vscode/extensions/ms-toolsai.jupyter-2021.2.576440691/out/client/extension.js:1:223195)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:94:5)",
      "at async L.getKernelForLocalRawConnection (/Users/jaredgaralde/.vscode/extensions/ms-toolsai.jupyter-2021.2.576440691/out/client/extension.js:1:221739)"
     ]
    }
   ]
  },
  {
   "source": [
    "df_sentiment = pd.read_csv('563_df_sentiments.csv')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "df_sentiment = df_sentiment.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all words that begin with # and turn the results into a dataframe\n",
    "temp = df_sentiment['Tweet'].str.lower().str.extractall(r\"(#\\w+)\")\n",
    "temp.columns = ['unnamed']\n",
    "# Convert the multiple hashtag values into a list\n",
    "temp = temp.groupby(level = 0)['unnamed'].apply(list)\n",
    "# Save the result as a feature in the original dataset\n",
    "df_sentiment['hashtags'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_sentiment)):\n",
    "    if df_sentiment.loc[i, 'No_of_Retweets'] >= 4:\n",
    "        df_sentiment.loc[i, 'No_of_Retweets'] = 4\n",
    "\n",
    "for i in range(len(df_sentiment)):\n",
    "    if df_sentiment.loc[i, 'No_of_Likes'] >= 10:\n",
    "        df_sentiment.loc[i, 'No_of_Likes'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_df = df_sentiment.groupby(['No_of_Retweets', 'vaderSentiment']).vaderSentimentScores.agg(count='count').reset_index()\n",
    "like_df = df_sentiment.groupby(['No_of_Likes', 'vaderSentiment']).vaderSentimentScores.agg(count='count').reset_index()\n",
    "classify_df = df_sentiment.vaderSentiment.value_counts().reset_index()\n",
    "df_sentiment.Labels = df_sentiment.Labels.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes_dict = df_sentiment.groupby('No_of_Likes').vaderSentimentScores.agg(count='count').to_dict()['count']\n",
    "df_retweet_dict = df_sentiment.groupby('No_of_Retweets').vaderSentimentScores.agg(count='count').to_dict()['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(like_df)):\n",
    "  like_df.loc[i, 'Normalized_count'] = like_df.loc[i, 'count'] / df_likes_dict[like_df.loc[i, 'No_of_Likes']]\n",
    "\n",
    "for i in range(len(retweet_df)):\n",
    "  retweet_df.loc[i, 'Normalized_count'] = retweet_df.loc[i, 'count'] / df_retweet_dict[retweet_df.loc[i, 'No_of_Retweets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(df.Tweet)\n",
    "words = cv.get_feature_names()\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm_transpose = data_dtm.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_counts = scipy.sparse.csr_matrix(data_dtm_transpose)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "word2id = dict((k, v) for k, v in cv.vocabulary_.items())\n",
    "d = corpora.Dictionary()\n",
    "d.id2token = id2word\n",
    "d.token2id = word2id"
   ]
  },
  {
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda, corpus, d)\n",
    "vis"
   ]
  }
 ]
}